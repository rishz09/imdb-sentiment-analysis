{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = 'aclImdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {'pos': 1, 'neg': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "df = df.rename(columns={'0': 'review', '1': 'sentiment'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBy calling fit_transform() method, we constructed the vocabulary of the bag\\nof words model and transformed the following 3 sentences into sparse feature\\nvectors\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "An example showing how CountVectorizer() creates the bag of words model for\n",
    "text data\n",
    "'''\n",
    "count = CountVectorizer()\n",
    "docs = np.array(['The sun is shining',\n",
    "                 'The weather is sweet',\n",
    "                 'The sun is shining, the weather is sweet, and one and one is two'])\n",
    "\n",
    "bag = count.fit_transform(docs)\n",
    "\n",
    "'''\n",
    "By calling fit_transform() method, we constructed the vocabulary of the bag\n",
    "of words model and transformed the following 3 sentences into sparse feature\n",
    "vectors\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 6, 'sun': 4, 'is': 1, 'shining': 3, 'weather': 8, 'sweet': 5, 'and': 0, 'one': 2, 'two': 7}\n",
      "[[0 1 0 1 1 0 1 0 0]\n",
      " [0 1 0 0 0 1 1 0 1]\n",
      " [2 3 2 1 1 1 2 1 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nIndices in the matrix correspond to the value that corresponds to a key in\\nthe vocabulary.\\nEg: first index corresponds to 'and' as it has value = 0 in vocabulary\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(count.vocabulary_)\n",
    "print(bag.toarray())\n",
    "'''\n",
    "Indices in the matrix correspond to the value that corresponds to a key in\n",
    "the vocabulary.\n",
    "Eg: first index corresponds to 'and' as it has value = 0 in vocabulary\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 1 0 1 0 0]\n",
      " [0 1 0 0 0 1 1 0 1]\n",
      " [2 3 2 1 1 1 2 1 1]]\n",
      "\n",
      "[[0.   0.43 0.   0.56 0.56 0.   0.43 0.   0.  ]\n",
      " [0.   0.43 0.   0.   0.   0.56 0.43 0.   0.56]\n",
      " [0.5  0.45 0.5  0.19 0.19 0.19 0.3  0.25 0.19]]\n",
      "\n",
      "[1.0000000000000004, 1.0000000000000004, 1.0]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Assessing term frequency-inverse document frequency\n",
    "[Previously, only term frequency used]\n",
    "\n",
    "IMPORTANT: tf-idf evaluates the importance of a word in a document \n",
    "relative to a collection of documents\n",
    "\n",
    "Read page 252 of S Raschka book (new one) to understand the concept\n",
    "'''\n",
    "tfidf = TfidfTransformer(use_idf=True,\n",
    "                         smooth_idf=True, #adding 1 to denominator of formula of idf\n",
    "                         norm='l2')\n",
    "\n",
    "'''\n",
    "Sum of squares of vector elements is 1. The cosine similarity between \n",
    "two vectors is their dot product when l2 norm has been applied.\n",
    "'''\n",
    "tf_arr = count.fit_transform(docs).toarray()\n",
    "res = tfidf.fit_transform(count.fit_transform(docs)).toarray()\n",
    "np.set_printoptions(precision=2)\n",
    "print(f'{tf_arr}\\n\\n{res}\\n')\n",
    "\n",
    "sum_of_square = [sum(i**2 for i in row) for row in res]\n",
    "print(sum_of_square)\n",
    "\n",
    "np.set_printoptions(threshold=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>In the above output, we see that for third vector (third sentence), 'is' is most frequently present (3 times), however, it has a relatively small tf-idf. This is because since it is also present in the first and second document, it is unlikely to contain any useful discriminatory information</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is seven.<br /><br />Title (Brazil): Not Available'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Cleaning text data\n",
    "We see that this contains HTML tags and other things that might not be useful\n",
    "for sentiment analysis. So we have to remove them\n",
    "'''\n",
    "df.loc[0, 'review'][-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mType:\u001b[0m        module\n",
      "\u001b[1;31mString form:\u001b[0m <module 're' from 'c:\\\\Anaconda\\\\envs\\\\ML\\\\Lib\\\\re\\\\__init__.py'>\n",
      "\u001b[1;31mFile:\u001b[0m        c:\\anaconda\\envs\\ml\\lib\\re\\__init__.py\n",
      "\u001b[1;31mDocstring:\u001b[0m  \n",
      "Support for regular expressions (RE).\n",
      "\n",
      "This module provides regular expression matching operations similar to\n",
      "those found in Perl.  It supports both 8-bit and Unicode strings; both\n",
      "the pattern and the strings being processed can contain null bytes and\n",
      "characters outside the US ASCII range.\n",
      "\n",
      "Regular expressions can contain both special and ordinary characters.\n",
      "Most ordinary characters, like \"A\", \"a\", or \"0\", are the simplest\n",
      "regular expressions; they simply match themselves.  You can\n",
      "concatenate ordinary characters, so last matches the string 'last'.\n",
      "\n",
      "The special characters are:\n",
      "    \".\"      Matches any character except a newline.\n",
      "    \"^\"      Matches the start of the string.\n",
      "    \"$\"      Matches the end of the string or just before the newline at\n",
      "             the end of the string.\n",
      "    \"*\"      Matches 0 or more (greedy) repetitions of the preceding RE.\n",
      "             Greedy means that it will match as many repetitions as possible.\n",
      "    \"+\"      Matches 1 or more (greedy) repetitions of the preceding RE.\n",
      "    \"?\"      Matches 0 or 1 (greedy) of the preceding RE.\n",
      "    *?,+?,?? Non-greedy versions of the previous three special characters.\n",
      "    {m,n}    Matches from m to n repetitions of the preceding RE.\n",
      "    {m,n}?   Non-greedy version of the above.\n",
      "    \"\\\\\"     Either escapes special characters or signals a special sequence.\n",
      "    []       Indicates a set of characters.\n",
      "             A \"^\" as the first character indicates a complementing set.\n",
      "    \"|\"      A|B, creates an RE that will match either A or B.\n",
      "    (...)    Matches the RE inside the parentheses.\n",
      "             The contents can be retrieved or matched later in the string.\n",
      "    (?aiLmsux) The letters set the corresponding flags defined below.\n",
      "    (?:...)  Non-grouping version of regular parentheses.\n",
      "    (?P<name>...) The substring matched by the group is accessible by name.\n",
      "    (?P=name)     Matches the text matched earlier by the group named name.\n",
      "    (?#...)  A comment; ignored.\n",
      "    (?=...)  Matches if ... matches next, but doesn't consume the string.\n",
      "    (?!...)  Matches if ... doesn't match next.\n",
      "    (?<=...) Matches if preceded by ... (must be fixed length).\n",
      "    (?<!...) Matches if not preceded by ... (must be fixed length).\n",
      "    (?(id/name)yes|no) Matches yes pattern if the group with id/name matched,\n",
      "                       the (optional) no pattern otherwise.\n",
      "\n",
      "The special sequences consist of \"\\\\\" and a character from the list\n",
      "below.  If the ordinary character is not on the list, then the\n",
      "resulting RE will match the second character.\n",
      "    \\number  Matches the contents of the group of the same number.\n",
      "    \\A       Matches only at the start of the string.\n",
      "    \\Z       Matches only at the end of the string.\n",
      "    \\b       Matches the empty string, but only at the start or end of a word.\n",
      "    \\B       Matches the empty string, but not at the start or end of a word.\n",
      "    \\d       Matches any decimal digit; equivalent to the set [0-9] in\n",
      "             bytes patterns or string patterns with the ASCII flag.\n",
      "             In string patterns without the ASCII flag, it will match the whole\n",
      "             range of Unicode digits.\n",
      "    \\D       Matches any non-digit character; equivalent to [^\\d].\n",
      "    \\s       Matches any whitespace character; equivalent to [ \\t\\n\\r\\f\\v] in\n",
      "             bytes patterns or string patterns with the ASCII flag.\n",
      "             In string patterns without the ASCII flag, it will match the whole\n",
      "             range of Unicode whitespace characters.\n",
      "    \\S       Matches any non-whitespace character; equivalent to [^\\s].\n",
      "    \\w       Matches any alphanumeric character; equivalent to [a-zA-Z0-9_]\n",
      "             in bytes patterns or string patterns with the ASCII flag.\n",
      "             In string patterns without the ASCII flag, it will match the\n",
      "             range of Unicode alphanumeric characters (letters plus digits\n",
      "             plus underscore).\n",
      "             With LOCALE, it will match the set [0-9_] plus characters defined\n",
      "             as letters for the current locale.\n",
      "    \\W       Matches the complement of \\w.\n",
      "    \\\\       Matches a literal backslash.\n",
      "\n",
      "This module exports the following functions:\n",
      "    match     Match a regular expression pattern to the beginning of a string.\n",
      "    fullmatch Match a regular expression pattern to all of a string.\n",
      "    search    Search a string for the presence of a pattern.\n",
      "    sub       Substitute occurrences of a pattern found in a string.\n",
      "    subn      Same as sub, but also return the number of substitutions made.\n",
      "    split     Split a string by the occurrences of a pattern.\n",
      "    findall   Find all occurrences of a pattern in a string.\n",
      "    finditer  Return an iterator yielding a Match object for each match.\n",
      "    compile   Compile a pattern into a Pattern object.\n",
      "    purge     Clear the regular expression cache.\n",
      "    escape    Backslash all non-alphanumerics in a string.\n",
      "\n",
      "Each function other than purge and escape can take an optional 'flags' argument\n",
      "consisting of one or more of the following module constants, joined by \"|\".\n",
      "A, L, and U are mutually exclusive.\n",
      "    A  ASCII       For string patterns, make \\w, \\W, \\b, \\B, \\d, \\D\n",
      "                   match the corresponding ASCII character categories\n",
      "                   (rather than the whole Unicode categories, which is the\n",
      "                   default).\n",
      "                   For bytes patterns, this flag is the only available\n",
      "                   behaviour and needn't be specified.\n",
      "    I  IGNORECASE  Perform case-insensitive matching.\n",
      "    L  LOCALE      Make \\w, \\W, \\b, \\B, dependent on the current locale.\n",
      "    M  MULTILINE   \"^\" matches the beginning of lines (after a newline)\n",
      "                   as well as the string.\n",
      "                   \"$\" matches the end of lines (before a newline) as well\n",
      "                   as the end of the string.\n",
      "    S  DOTALL      \".\" matches any character at all, including the newline.\n",
      "    X  VERBOSE     Ignore whitespace and comments for nicer looking RE's.\n",
      "    U  UNICODE     For compatibility only. Ignored for string patterns (it\n",
      "                   is the default), and forbidden for bytes patterns.\n",
      "\n",
      "This module also defines an exception 'error'."
     ]
    }
   ],
   "source": [
    "re?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text) #removes any HTML tag\n",
    "    #finding emoticons as these are essential to sentiment analysis\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "\n",
    "    #[\\W]+ denotes non-alphanumeric characters \n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) + \n",
    "            ' '.join(emoticons).replace('-', ''))\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is seven.<br /><br />Title (Brazil): Not Available\n",
      "is seven title brazil not available\n",
      "this is a test :) :( :)\n"
     ]
    }
   ],
   "source": [
    "#checking the preprocessor\n",
    "print(df.loc[0, 'review'][-50:])\n",
    "print(preprocessor(df.loc[0, 'review'][-50:]))\n",
    "print(preprocessor('</a>This :) is :( a test :-)!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['runners', 'like', 'running', 'and', 'thus', 'they', 'run']\n"
     ]
    }
   ],
   "source": [
    "#processing documents into tokens\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "print(tokenizer('runners like running and thus they run'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "word stemming - related to tokenization\n",
    "It is the process of transforming a word into its root form. Allows us to map\n",
    "related words to the same stem. Developed by Martin F. Porter and is known as\n",
    "Porter stemmer algorithm\n",
    "'''\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "print(tokenizer_porter('runners like running and thus they run'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "len(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'run', 'lot']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example\n",
    "[w for w in tokenizer_porter('a runner likes running and runs a lot')\n",
    "                             if w not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now classifying using 25000 training and 25000 test examples\n",
    "x_train = df.loc[:25000, 'review'].values\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "x_test = df.loc[25000:, 'review'].values\n",
    "y_test = df.loc[25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 6, 'sun': 4, 'is': 1, 'shining': 3, 'weather': 8, 'sweet': 5, 'and': 0, 'one': 2, 'two': 7}\n",
      "[[0.   0.43 0.   0.56 0.56 0.   0.43 0.   0.  ]\n",
      " [0.   0.43 0.   0.   0.   0.56 0.43 0.   0.56]\n",
      " [0.5  0.45 0.5  0.19 0.19 0.19 0.3  0.25 0.19]]\n"
     ]
    }
   ],
   "source": [
    "#TfidfVectorizer combines CountVectorizer and TfidfTransformer\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=True,\n",
    "                        preprocessor=None)\n",
    "\n",
    "docs = np.array(['The sun is shining',\n",
    "                 'The weather is sweet',\n",
    "                 'The sun is shining, the weather is sweet, and one and one is two'])\n",
    "\n",
    "arr = tfidf.fit_transform(docs).toarray()\n",
    "\n",
    "print(tfidf.vocabulary_)\n",
    "print(arr)\n",
    "\n",
    "#same as using CountVectorizer and TfidfTransformer sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\envs\\ML\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                                        TfidfVectorizer(lowercase=False)),\n",
       "                                       (&#x27;lr&#x27;,\n",
       "                                        LogisticRegression(solver=&#x27;liblinear&#x27;))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{&#x27;lr__C&#x27;: [1.0, 10.0], &#x27;lr__penalty&#x27;: [&#x27;l2&#x27;],\n",
       "                          &#x27;vect__ngram_range&#x27;: [(1, 1)],\n",
       "                          &#x27;vect__stop_words&#x27;: [None],\n",
       "                          &#x27;vect__tokenizer&#x27;: [&lt;function tokenizer at 0x000001E2E91DF740&gt;,\n",
       "                                              &lt;function tokenizer_porter at 0x000001E2EE6...\n",
       "                          &#x27;vect__stop_words&#x27;: [[&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;,\n",
       "                                                &#x27;our&#x27;, &#x27;ours&#x27;, &#x27;ourselves&#x27;,\n",
       "                                                &#x27;you&#x27;, &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;,\n",
       "                                                &quot;you&#x27;ll&quot;, &quot;you&#x27;d&quot;, &#x27;your&#x27;,\n",
       "                                                &#x27;yours&#x27;, &#x27;yourself&#x27;,\n",
       "                                                &#x27;yourselves&#x27;, &#x27;he&#x27;, &#x27;him&#x27;,\n",
       "                                                &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;,\n",
       "                                                &quot;she&#x27;s&quot;, &#x27;her&#x27;, &#x27;hers&#x27;,\n",
       "                                                &#x27;herself&#x27;, &#x27;it&#x27;, &quot;it&#x27;s&quot;, &#x27;its&#x27;,\n",
       "                                                &#x27;itself&#x27;, ...],\n",
       "                                               None],\n",
       "                          &#x27;vect__tokenizer&#x27;: [&lt;function tokenizer at 0x000001E2E91DF740&gt;],\n",
       "                          &#x27;vect__use_idf&#x27;: [False]}],\n",
       "             scoring=&#x27;accuracy&#x27;, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                                        TfidfVectorizer(lowercase=False)),\n",
       "                                       (&#x27;lr&#x27;,\n",
       "                                        LogisticRegression(solver=&#x27;liblinear&#x27;))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{&#x27;lr__C&#x27;: [1.0, 10.0], &#x27;lr__penalty&#x27;: [&#x27;l2&#x27;],\n",
       "                          &#x27;vect__ngram_range&#x27;: [(1, 1)],\n",
       "                          &#x27;vect__stop_words&#x27;: [None],\n",
       "                          &#x27;vect__tokenizer&#x27;: [&lt;function tokenizer at 0x000001E2E91DF740&gt;,\n",
       "                                              &lt;function tokenizer_porter at 0x000001E2EE6...\n",
       "                          &#x27;vect__stop_words&#x27;: [[&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;,\n",
       "                                                &#x27;our&#x27;, &#x27;ours&#x27;, &#x27;ourselves&#x27;,\n",
       "                                                &#x27;you&#x27;, &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;,\n",
       "                                                &quot;you&#x27;ll&quot;, &quot;you&#x27;d&quot;, &#x27;your&#x27;,\n",
       "                                                &#x27;yours&#x27;, &#x27;yourself&#x27;,\n",
       "                                                &#x27;yourselves&#x27;, &#x27;he&#x27;, &#x27;him&#x27;,\n",
       "                                                &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;,\n",
       "                                                &quot;she&#x27;s&quot;, &#x27;her&#x27;, &#x27;hers&#x27;,\n",
       "                                                &#x27;herself&#x27;, &#x27;it&#x27;, &quot;it&#x27;s&quot;, &#x27;its&#x27;,\n",
       "                                                &#x27;itself&#x27;, ...],\n",
       "                                               None],\n",
       "                          &#x27;vect__tokenizer&#x27;: [&lt;function tokenizer at 0x000001E2E91DF740&gt;],\n",
       "                          &#x27;vect__use_idf&#x27;: [False]}],\n",
       "             scoring=&#x27;accuracy&#x27;, verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, TfidfVectorizer(lowercase=False)),\n",
       "                (&#x27;lr&#x27;, LogisticRegression(solver=&#x27;liblinear&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(lowercase=False)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('vect',\n",
       "                                        TfidfVectorizer(lowercase=False)),\n",
       "                                       ('lr',\n",
       "                                        LogisticRegression(solver='liblinear'))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{'lr__C': [1.0, 10.0], 'lr__penalty': ['l2'],\n",
       "                          'vect__ngram_range': [(1, 1)],\n",
       "                          'vect__stop_words': [None],\n",
       "                          'vect__tokenizer': [<function tokenizer at 0x000001E2E91DF740>,\n",
       "                                              <function tokenizer_porter at 0x000001E2EE6...\n",
       "                          'vect__stop_words': [['i', 'me', 'my', 'myself', 'we',\n",
       "                                                'our', 'ours', 'ourselves',\n",
       "                                                'you', \"you're\", \"you've\",\n",
       "                                                \"you'll\", \"you'd\", 'your',\n",
       "                                                'yours', 'yourself',\n",
       "                                                'yourselves', 'he', 'him',\n",
       "                                                'his', 'himself', 'she',\n",
       "                                                \"she's\", 'her', 'hers',\n",
       "                                                'herself', 'it', \"it's\", 'its',\n",
       "                                                'itself', ...],\n",
       "                                               None],\n",
       "                          'vect__tokenizer': [<function tokenizer at 0x000001E2E91DF740>],\n",
       "                          'vect__use_idf': [False]}],\n",
       "             scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using GridSearchCV to find optimal params for logistic regression model\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "small_param_grid = [\n",
    "    {\n",
    "        #using only unigrams\n",
    "        'vect__ngram_range': [(1, 1)],\n",
    "        'vect__stop_words': [None],\n",
    "        'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "        #checking both tokenizer and tokenizer_porter (word stemming) cases \n",
    "        'lr__penalty': ['l2'],\n",
    "        'lr__C': [1.0, 10.0]\n",
    "    },\n",
    "    {\n",
    "        'vect__ngram_range': [(1, 1)],\n",
    "        #removing stop words \n",
    "        'vect__stop_words': [stop, None],\n",
    "        'vect__tokenizer': [tokenizer],\n",
    "        'vect__use_idf': [False],\n",
    "        'vect__smooth_idf': [False],\n",
    "        'vect__norm': [None],\n",
    "        #user_idf = False, smooth_idf = False and norm = None implies that \n",
    "        #we are training model based on raw term frequencies (tf)\n",
    "        'lr__penalty': ['l2'],\n",
    "        'lr__C': [1.0, 10.0]\n",
    "    }\n",
    "]\n",
    "\n",
    "lr_tfidf = Pipeline([\n",
    "    ('vect', tfidf),\n",
    "    ('lr', LogisticRegression(solver='liblinear'))\n",
    "])\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(estimator=lr_tfidf,\n",
    "                           param_grid=small_param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5,\n",
    "                           verbose=2,\n",
    "                           n_jobs=-1)\n",
    "\n",
    "gs_lr_tfidf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr__C': 10.0, 'lr__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x000001E2E91DF740>}\n"
     ]
    }
   ],
   "source": [
    "print(gs_lr_tfidf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy: 0.8970842631473704\n"
     ]
    }
   ],
   "source": [
    "print(f'CV Accuracy: {gs_lr_tfidf.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.89876\n"
     ]
    }
   ],
   "source": [
    "clf = gs_lr_tfidf.best_estimator_\n",
    "print(f'Test Accuracy: {clf.score(x_test, y_test)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
